box_plot_Time
)
# Plot the predictors as box plots
predictor_cols <- cols[!cols %in% c('Class')]
for(i in predictor_cols){
assign(paste('box_plot_',i,sep=""),
ggplot() + geom_boxplot(data=df_cc_train, aes_string(x="Class", y=i)) + theme(text = element_text(size = 10))
)
}
ggarrange(
box_plot_V1,
box_plot_V2,
box_plot_V3,
box_plot_V4,
box_plot_V5,
box_plot_V6,
box_plot_V7,
box_plot_V8,
box_plot_V9,
box_plot_V10,
box_plot_V11,
box_plot_V12,
box_plot_V13,
box_plot_V14,
box_plot_V15,
box_plot_V16,
box_plot_V17,
box_plot_V18,
box_plot_V19,
box_plot_V20,
box_plot_V21,
box_plot_V22,
box_plot_V23,
box_plot_V24,
box_plot_V25,
box_plot_V26,
box_plot_V27,
box_plot_V28,
box_plot_Amount,
box_plot_Time
)
# Pulling predictors into a separate dataframe.
preds <- df_cc_train[, -31]
# Scaling the predictors.
preds_scaled <- as.data.frame(scale(preds, center = TRUE, scale = TRUE))
cbind(melt(apply(preds_scaled, 2, min), value.name = "min"),
melt(apply(preds_scaled, 2, mean), value.name = "mean"),
melt(apply(preds_scaled, 2, max), value.name = "max"))
df_cc_train_scaled <- cbind(preds_scaled, Class=df_cc_train$Class)
# Create training set with 75% negative cases and 25% positive cases
# Create dataframe of negative cases
train_neg <- df_cc_train[df_cc_train["Class"] == 0,]
# Sample 1047 negative cases
set.seed(508)
neg_sample <- train_neg[sample(1:nrow(train_neg),1047),]
# Combine with positive cases
downsample_25 <- rbind(neg_sample, df_cc_train[df_cc_train["Class"] == 1,])
table(downsample_25["Class"])
# Created scaled versioin
downsampled_25_scaled <- as.data.frame(scale(downsample_25[,-31], center = TRUE, scale = TRUE))
downsampled_25_scaled <- cbind(downsampled_25_scaled, Class=downsample_25$Class)
validation_true <- df_cc_validation$Class
set.seed(508)
nb.fit <- naiveBayes(Class ~ ., data = downsample_25)
validation_pred <- predict(nb.fit, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.F1 <- nb.confusion[[4]][7])
# Downsampled
set.seed(508)
downsample_50 <- downSample(df_cc_train[,-31], df_cc_train$Class)
# Downsampled and Scaled
set.seed(508)
downsample_scaled <- downSample(df_cc_train_scaled[,-31], df_cc_train_scaled$Class)
set.seed(1)
# SMOTE
smote_cc_train <- SMOTE(X = df_cc_train[, -31], target = df_cc_train$Class, K = 5)
# SMOTE and Scaled
set.seed(1)
smote_cc_train_scaled <- SMOTE(X = df_cc_train_scaled[, -31], target = df_cc_train_scaled$Class, K = 5)
df_cc_validation_scaled_x <- as.data.frame(scale(df_cc_validation[,-31], center = TRUE, scale = TRUE))
df_cc_validation_scaled <- cbind(df_cc_validation_scaled_x, Class=df_cc_validation$Class)
validation_true <- df_cc_validation$Class
set.seed(508)
svc_sample <- smote_cc_train$data[sample(1:nrow(smote_cc_train$data),2000),]
set.seed(508)
tune.out <- tune (svm, as.factor(class) ~ ., data = svc_sample, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
), type = "C"
)
summary(tune.out)
best_svm_smote <- tune.out$best.model
validation_pred <- predict(best_svm_smote, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.FI <- svm.smote.confusion[[4]][7])
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
tree.mod
summary(tree.mod)
library(tree)
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
str(df_cc_train)
tree(Class ~., data = df_cc_train)
detach("package:dplyr", unload=TRUE)
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(caret) # Used for confusionMatrix
library(class)
library(data.table) # Used for melt()
library(dplyr)
library(e1071)
library(ggplot2)
library(ggpubr)
library(glmnet) # Used for Logistic Regression Lasso
library(MASS) # Used for LDA/QDA
library(smotefamily) # Used for SMOTE()
library(tree)
creditcard <- read.csv('creditcard.csv')
creditcard[, 'Class'] <- as.factor(creditcard[, 'Class'])
set.seed(508)
spec = c(train = .7, test = .15, validate = .15)
g = sample(cut(
seq(nrow(creditcard)),
nrow(creditcard)*cumsum(c(0,spec)),
labels = names(spec)
))
res = split(creditcard, g)
# Setting the training dataframe
df_cc_train <- res$train
# Setting the testing dataframe
df_cc_test <- res$test
# Setting the validation dataframe
df_cc_validation <- res$validate
# Pulling predictors into a separate dataframe.
preds <- df_cc_train[, -31]
# Scaling the predictors.
preds_scaled <- as.data.frame(scale(preds, center = TRUE, scale = TRUE))
cbind(melt(apply(preds_scaled, 2, min), value.name = "min"),
melt(apply(preds_scaled, 2, mean), value.name = "mean"),
melt(apply(preds_scaled, 2, max), value.name = "max"))
df_cc_train_scaled <- cbind(preds_scaled, Class=df_cc_train$Class)
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
tree:::print.tree(tree.mod)
### plot tree model
plot(tree.mod)
text(tree.mod, pretty = 0)
### cross-validate to see if best subset model
set.seed(329)
cv.tree.mod <- cv.tree(tree.mod, FUN = prune.misclass)
names(cv.tree.mod)
cv.tree.mod
par(mfrow = c(1,2))
plot(cv.tree.mod$size, cv.tree.mod$dev, type = "b")
plot(cv.tree.mod$k, cv.tree.mod$dev, type = "b")
#best could be 5 or 6 terminal nodes
plot(prune.tree.mod)
### subset original model with 6 terminal nodes
prune.tree.mod <- prune.misclass(tree.mod, best = 6)
plot(prune.tree.mod)
text(prune.tree.mod, pretty = 0)
#this is same model so we'll use orig model
### predict on validation set
tree.pred <- predict(tree.mod, df_cc_validation, type = "class")
(tree.confusion <- confusionMatrix(tree.pred, df_cc_validation_class,
mode="everything", positive = "1"))
df_cc_validation_class <- df_cc_validation$Class
### predict on validation set
tree.pred <- predict(tree.mod, df_cc_validation, type = "class")
(tree.confusion <- confusionMatrix(tree.pred, df_cc_validation_class,
mode="everything", positive = "1"))
(tree.F1 <- tree.confusion[[4]][7])
set.seed(508)
smote_cc_train <- SMOTE(X = dplyr::select(df_cc_train_scaled, -Class), target = df_cc_train_scaled$Class, K = 5)
table(smote_cc_train$data$class)
x.train <- data.matrix(subset(smote_cc_train$data, select = -c(class)))
y.train <- data.matrix(smote_cc_train$data[, c('class')])
logistic.fit <- cv.glmnet(x.train, y.train, family = "binomial", alpha = 1)
bestlam <- logistic.fit$lambda.min
x.validation <- data.matrix(scale(subset(df_cc_validation, select = -c(Class))))
y.validation <- data.matrix(df_cc_validation[, c('Class')])
logistic.preds <- predict(logistic.fit, s = bestlam, newx = x.validation, type = "class")
logistic.confusion <- confusionMatrix(as.factor(logistic.preds), as.factor(y.validation), mode="everything", positive = "1")
logistic.confusion
logistic.F1 <- logistic.confusion[[4]][7]
lda.fit <- lda(class ~ ., data = smote_cc_train$data)
lda.fit
lda.pred <- predict(lda.fit, as.data.frame(x.validation))
lda.class <- lda.pred$class
lda.confusion <- confusionMatrix(as.factor(lda.class), as.factor(y.validation), mode="everything", positive = "1")
lda.confusion
lda.F1 <- lda.confusion[[4]][7]
qda.fit <- qda(class ~ ., data = smote_cc_train$data)
qda.fit
qda.pred <- predict(lda.fit, as.data.frame(x.validation))
qda.class <- qda.pred$class
qda.confusion <- confusionMatrix(as.factor(qda.class), as.factor(y.validation), mode="everything", positive = "1")
qda.confusion
qda.F1 <- qda.confusion[[4]][7]
(svm.smote.F1 <- svm.smote.confusion[[4]][7])
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, nb.orig.F1, nb.scale.F1,
nb.smote.F1, svm.down.F1, svm.down.scale.F1, svm.smote.F1,
svm.smote.scale.F1, tree.F1, knn.F1))
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, svm.down.F1, svm.down.scale.F1, svm.smote.F1,
tree.F1, knn.F1))
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, svm.smote.F1,
tree.F1, knn.F1))
########## KNN Model ##########
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
########## KNN Model ##########
df_cc_train_mat <- df_cc_train[,!names(df_cc_train) %in% c("Class")]
df_cc_train_mat <- scale(df_cc_train_mat)
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
########## KNN Model ##########
df_cc_train_mat <- df_cc_train[,!names(df_cc_train) %in% c("Class")]
df_cc_train_mat <- scale(df_cc_train_mat)
df_cc_validation_mat <- df_cc_validation[, !names(df_cc_validation) %in%
c("Class")]
df_cc_validation_mat <- scale(df_cc_validation_mat)
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
########## KNN Model ##########
df_cc_train_class <- df_cc_train$Class
df_cc_train_mat <- df_cc_train[,!names(df_cc_train) %in% c("Class")]
df_cc_train_mat <- scale(df_cc_train_mat)
df_cc_validation_mat <- df_cc_validation[, !names(df_cc_validation) %in%
c("Class")]
df_cc_validation_mat <- scale(df_cc_validation_mat)
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
(knn.confusion <- confusionMatrix(knn.pred, df_cc_validation_class,
mode="everything", positive = "1"))
(knn.F1 <- knn.confusion[[4]][7])
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, svm.smote.F1,
tree.F1, knn.F1))
# Two highest F1 scores are tree and knn
### Tree Model
tree.pred.test <- predict(tree.mod, df_cc_test, type = "class")
(tree.test.confusion <- confusionMatrix(tree.pred.test, df_cc_test_class,
mode="everything", positive = "1"))
# split predictors from response for test set
df_cc_test_class <- df_cc_test$Class
df_cc_test_mat <- df_cc_test[, !names(df_cc_test) %in% c("Class")]
df_cc_test_mat <- scale(df_cc_test_mat)
### Tree Model
tree.pred.test <- predict(tree.mod, df_cc_test, type = "class")
(tree.test.confusion <- confusionMatrix(tree.pred.test, df_cc_test_class,
mode="everything", positive = "1"))
(tree.test.F1 <- tree.test.confusion[[4]][7])
### KNN Model
set.seed(506)
knn.pred.test <- knn(df_cc_train_mat, df_cc_test_mat, df_cc_train_class, k = 1)
table(knn.pred.test , df_cc_test_class)
(knn.test.confusion <- confusionMatrix(knn.pred.test, df_cc_test_class,
mode="everything", positive = "1"))
(knn.test.F1 <- knn.test.confusion[[4]][7])
### final comparison
(model_table_final <- rbind(tree.test.F1, knn.test.F1,tree.F1, knn.F1))
##### Model Comparisons by F1 Score #####
(model_table <- rbind(knn.F1, tree.F1, svm.smote.F1, lda.F1, qda.F1, nb.F1, logistic.F1)) # Sorted in descending F1 scores.
# Two highest F1 scores are tree and knn
### final comparison
(model_table_final <- rbind(tree.test.F1, knn.test.F1,tree.F1, knn.F1))
### final comparison
(model_table_final <- rbind(knn.F1, knn.test.F1, tree.F1, tree.test.F1))
set.seed(508)
nb.fit.orig <- naiveBayes(Class ~ ., data = df_cc_train)
validation_pred <- predict(nb.fit.orig, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode="everything", positive = "1")
(nb.orig.confusion <- confusionMatrix(validation_pred, validation_true, mode="everything", positive = "1"))
(nb.orig.F1 <- nb.orig.confusion[[4]][7])
set.seed(508)
nb.fit.scaled <- naiveBayes(Class ~ ., data = downsampled_25_scaled)
validation_pred <- predict(nb.fit.scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.scale.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.scale.F1 <- nb.scale.confusion[[4]][7])
nb.fit.smote <- naiveBayes(class ~ ., data = smote_cc_train$data)
validation_pred <- predict(nb.fit.smote, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.smote <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.smote.F1 <- nb.smote[[4]][7])
nb.fit.smote <- naiveBayes(class ~ ., data = smote_cc_train_scaled$data)
validation_pred <- predict(nb.fit.smote, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.smote <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.smote.F1 <- nb.smote[[4]][7])
# Downsampled
set.seed(508)
downsample_50 <- downSample(df_cc_train[,-31], df_cc_train$Class)
# Downsampled and Scaled
set.seed(508)
downsample_scaled <- downSample(df_cc_train_scaled[,-31], df_cc_train_scaled$Class)
set.seed(1)
# SMOTE
smote_cc_train <- SMOTE(X = df_cc_train[, -31], target = df_cc_train$Class, K = 5)
# SMOTE and Scaled
set.seed(1)
smote_cc_train_scaled <- SMOTE(X = df_cc_train_scaled[, -31], target = df_cc_train_scaled$Class, K = 5)
df_cc_validation_scaled_x <- as.data.frame(scale(df_cc_validation[,-31], center = TRUE, scale = TRUE))
df_cc_validation_scaled <- cbind(df_cc_validation_scaled_x, Class=df_cc_validation$Class)
validation_true <- df_cc_validation$Class
set.seed(508)
tune.out <- tune(svm, Class ~ ., data = downsample_50, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
best_svc_downsample <- tune.out$best.model
validation_pred <- predict(best_svc_downsample, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.down.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.down.F1 <- svm.down.confusion[[4]][7])
set.seed(508)
tune.out <- tune(svm, Class ~ ., data = downsample_scaled, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
best_svc_ds_scaled <- tune.out$best.model
validation_pred <- predict(best_svc_ds_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.down.scale.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.down.scale.F1 <- svm.down.confusion[[4]][7])
set.seed(508)
svc_sample <- smote_cc_train$data[sample(1:nrow(smote_cc_train$data),2000),]
set.seed(508)
tune.out <- tune(svm, as.factor(class) ~ ., data = svc_sample, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)), type = "C")
summary(tune.out)
best_svc_smote <- tune.out$best.model
validation_pred <- predict(best_svc_smote, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.FI <- svm.smote.confusion[[4]][7])
set.seed(508)
svc_sample <- smote_cc_train_scaled$data[sample(1:nrow(smote_cc_train_scaled$data),2000),]
set.seed(508)
tune.out <- tune(svm, as.factor(class) ~ ., data = svc_sample, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)), type = "C")
summary(tune.out)
best_svc_smote_scaled <- tune.out$best.model
validation_pred <- predict(best_svc_smote_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
set.seed(508)
tune.out <- tune (svm, Class ~ ., data = downsample_50, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
)
)
summary(tune.out)
best_svm_downsample <- tune.out$best.model
validation_pred <- predict(best_svm_downsample, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.scale.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.scale.F1 <- svm.smote.confusion[[4]][7])
set.seed(508)
tune.out <- tune (svm, Class ~ ., data = downsample_scaled, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
)
)
summary(tune.out)
best_svm_ds_scaled <- tune.out$best.model
validation_pred <- predict(best_svm_ds_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.down.scale.radial.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
#MEK COMMENT: assign F1 score to variable
(svm.down.scale.radial.F1 <- svm.smote.confusion[[4]][7])
set.seed(508)
svc_sample <- smote_cc_train_scaled$data[sample(1:nrow(smote_cc_train_scaled$data),2000),]
set.seed(508)
tune.out <- tune (svm, as.factor(class) ~ ., data = svc_sample, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
), type = "C"
)
summary(tune.out)
best_svm_smote_scaled <- tune.out$best.model
validation_pred <- predict(best_svm_smote_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.scale.radial.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.scale.radial.F1 <- svm.smote.confusion[[4]][7])
gc()
library(caret) # For dummyVars()
library(glmnet) # For Ridge Regression and Lasso
library(ggpubr)
library(leaps) # For regsubsets()
library(tidyverse)
# Read in the source data files.
CollegeCr <- read.csv('../Data/CollegeCr.csv')
CollegeCr_test <- read.csv('../Data/CollegeCr.test.csv')
setwd("~/School/STAT 580/Project 2/STAT580_Project2/Code")
# Read in the source data files.
CollegeCr <- read.csv('../Data/CollegeCr.csv')
CollegeCr_test <- read.csv('../Data/CollegeCr.test.csv')
Edwards <- read.csv('../Data/Edwards.csv')
Edwards_test <- read.csv('../Data/Edwards.test.csv')
OldTown <- read.csv('../Data/OldTown.csv')
OldTown_test <- read.csv('../Data/OldTown.test.csv')
# Find the common column names between the files to combine.
intersect_colnames <- sort(Reduce(intersect,list(colnames(CollegeCr),colnames(Edwards),colnames(OldTown))))
intersect_colnames_test <- sort(Reduce(intersect,list(colnames(CollegeCr_test),colnames(Edwards_test),colnames(OldTown_test))))
# Combine the neighborhoods into a common dataframe.
# Retain their neighborhood name into a variable named `Neighborhood`
df_neighborhoods <- rbind(CollegeCr[,intersect_colnames] %>% mutate(Neighborhood = "CollegeCr"),
Edwards[,intersect_colnames] %>% mutate(Neighborhood = "Edwards")
,OldTown[,intersect_colnames] %>% mutate(Neighborhood = "OldTown"))
df_neighborhoods_test <- rbind(CollegeCr_test[,intersect_colnames_test] %>% mutate(Neighborhood = "CollegeCr"),
Edwards_test[,intersect_colnames_test] %>% mutate(Neighborhood = "Edwards")
,OldTown_test[,intersect_colnames_test] %>% mutate(Neighborhood = "OldTown"))
unique(df_neighborhoods$YearBuilt)
unique(df_neighborhoods$YrSold)
sort(unique(df_neighborhoods$YearBuilt))
# The training/test split between neighborhoods is slightly imbalanced.
nrow(CollegeCr) / (nrow(CollegeCr) + nrow(CollegeCr_test)) # 0.7945205
nrow(Edwards) / (nrow(Edwards) + nrow(Edwards_test)) # 0.8314607
nrow(OldTown) / (nrow(OldTown) + nrow(OldTown_test)) # 0.8018018
# Separate the columns with multiple delimited values (`Exterior` and `LotInfo`) into separate columns.
exterior_cols = c("Exterior1st","ExteriorQual","ExteriorCond")
lot_cols = c("LotConfig","LotShape","LotArea","LotFrontage")
df_neighborhoods_separate <- df_neighborhoods %>% separate(Exterior, exterior_cols, sep=";") %>% separate(LotInfo, lot_cols, sep=";")
df_neighborhoods_separate_test <- df_neighborhoods_test %>% separate(Exterior, exterior_cols, sep=";") %>% separate(LotInfo, lot_cols, sep=";")
df_neighborhoods_separate[,c("LotArea","LotFrontage")] <- sapply(df_neighborhoods_separate[,c("LotArea","LotFrontage")], as.integer)
df_neighborhoods_separate_test[,c("LotArea","LotFrontage")] <- sapply(df_neighborhoods_separate_test[,c("LotArea","LotFrontage")], as.integer)
# De-duplicating records (rows 115, 116, and 279 are duplicates). We only perform this for training data because we do not want to remove records from test data.
df_neighborhoods_dedupe <- unique(df_neighborhoods_separate)
# Fill in "NA" for empty strings in `BsmtQual`, `BsmtFinType1`, and `GarageType`.
# Also replace the empty `LotFR3` columns with 0. We will assume they do not have frontage on 3 sides.
empty_string_cols <- c("BsmtQual","BsmtFinType1","GarageType")
df_neighborhoods_impute <- df_neighborhoods_dedupe %>%
mutate_at(empty_string_cols, ~replace(., . == "", "NA")) %>%
mutate_at("BsmtCond", ~replace(., is.na(.), "NA")) %>%
mutate_at("LotFrontage", ~replace(., is.na(.), 0)
)
df_neighborhoods_impute_test <- df_neighborhoods_separate_test %>%
mutate_at(empty_string_cols, ~replace(., . == "", "NA")) %>%
mutate_at("BsmtCond", ~replace(., is.na(.), "NA")) %>%
mutate_at("LotFrontage", ~replace(., is.na(.), 0)
)
# Plot histograms for numeric columns
num_cols <- colnames(select_if(df_neighborhoods_impute, is.numeric))
for(i in num_cols){
assign(paste('plot_',i,sep=""),
ggplot(data=df_neighborhoods_impute, aes_string(x=i)) + geom_histogram() + theme(text = element_text(size = 8))
)
}
ggarrange(
plot_BedroomAbvGr,
plot_BsmtFinSF1,
plot_Fireplaces,
plot_FullBath,
plot_GrLivArea,
plot_HalfBath,
plot_LotArea,
plot_LotFrontage,
plot_OpenPorchSF,
plot_OverallCond,
plot_OverallQual,
plot_SalePrice,
plot_TotRmsAbvGrd,
plot_WoodDeckSF,
plot_YearBuilt,
plot_YrSold
)
# Removing the record with `YrSold` of 2001 because it is an error.
# Also removing the `Utilities` column because it only has one value.
df_neighborhoods_cleaned <- df_neighborhoods_impute[-which(df_neighborhoods_impute$YrSold == 2001) , !names(df_neighborhoods_impute) == "Utilities"]
df_neighborhoods_cleaned_test <- df_neighborhoods_impute_test[ , !names(df_neighborhoods_impute) == "Utilities"] # There is no `YrSold` == 2001 record to remove from the test data.
# Encodes the categorical variables
dmy <- dummyVars(" ~ .", data = df_neighborhoods_cleaned[, !names(df_neighborhoods_cleaned) == "SalePrice"])
df_neighborhoods_final <- cbind(data.frame(predict(dmy, newdata = df_neighborhoods_cleaned)), df_neighborhoods_cleaned$SalePrice)
names(df_neighborhoods_final)[names(df_neighborhoods_final) == "df_neighborhoods_cleaned$SalePrice"] <- "SalePrice"
# Pulling a list of dummy variable column names so we can exclude them from standardization.
dummyVarNames <- names(df_neighborhoods_final)[!(names(df_neighborhoods_final) %in% names(df_neighborhoods_cleaned))]
# Creating scaled versions for Ridge Regression and Lasso, excluding the dummy variable names
df_neighborhoods_scaled <- cbind(df_neighborhoods_cleaned[, -which(names(df_neighborhoods_cleaned) %in% c('SalePrice'))] %>% mutate_if(is.numeric, scale) %>% mutate_if(is.character, as.factor), df_neighborhoods_cleaned$SalePrice)
names(df_neighborhoods_scaled)[names(df_neighborhoods_scaled) == "df_neighborhoods_cleaned$SalePrice"] <- "SalePrice"
df_neighborhoods_scaled_test <- df_neighborhoods_cleaned_test[, -which(names(df_neighborhoods_cleaned_test) %in% c('SalePrice'))] %>% mutate_if(is.numeric, scale) %>% mutate_if(is.character, as.factor)
num_cols <- colnames(select_if(df_neighborhoods_scaled, is.numeric))
df_neighborhoods_scaled
dim(df_neighborhoods_scaled)
dim(df_neighborhoods_final)
cbind(df_neighborhoods_final$YearBuilt, df_neighborhoods_scaled$YearBuilt)
ggplot(cbind(df_neighborhoods_final$YearBuilt, df_neighborhoods_scaled$YearBuilt), aes(x=df_neighborhoods_final$YearBuilt, y = df_neighborhoods_scaled$YearBuilt)) + geom_point()
ggplot(data.frame(cbind(df_neighborhoods_final$YearBuilt, df_neighborhoods_scaled$YearBuilt)), aes(x=df_neighborhoods_final$YearBuilt, y = df_neighborhoods_scaled$YearBuilt)) + geom_point()
ggplot(data.frame(cbind(df_neighborhoods_final$YrSold, df_neighborhoods_scaled$YrSold)), aes(x=df_neighborhoods_final$YearBuilt, y = df_neighborhoods_scaled$YearBuilt)) + geom_point()
ggplot(data.frame(cbind(df_neighborhoods_final$YrSold, df_neighborhoods_scaled$YrSold)), aes(x=df_neighborhoods_final$YrSold, y = df_neighborhoods_scaled$YrSold)) + geom_point()
