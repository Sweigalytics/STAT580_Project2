# Setting the testing dataframe
df_cc_test <- res$test
# Setting the validation dataframe
df_cc_validation <- res$validate
head(creditcard)
dim(creditcard)
summary(creditcard)
cols = names(df_cc_train)
# Plot the columns as histograms.
for(i in cols){
assign(paste('plot_',i,sep=""),
ggplot(data=df_cc_train, aes_string(x=i)) + geom_histogram() + theme(text = element_text(size = 8))
)
}
# Plot Class as a bar chart because it is a Factor.
plot_Class <- ggplot(data=df_cc_train, aes(x=Class)) + geom_bar() + theme(text = element_text(size = 8))
ggarrange(
plot_Time,
plot_V1,
plot_V2,
plot_V3,
plot_V4,
plot_V5,
plot_V6,
plot_V7,
plot_V8,
plot_V9,
plot_V10,
plot_V11,
plot_V12,
plot_V13,
plot_V14,
plot_V15,
plot_V16,
plot_V17,
plot_V18,
plot_V19,
plot_V20,
plot_V21,
plot_V22,
plot_V23,
plot_V24,
plot_V25,
plot_V26,
plot_V27,
plot_V28,
plot_Amount,
plot_Class
)
# Plot the predictors as box plots
predictor_cols <- cols[!cols %in% c('Class')]
for(i in predictor_cols){
assign(paste('box_plot_',i,sep=""),
ggplot() + geom_boxplot(data=df_cc_train, aes_string(x="Class", y=i)) + theme(text = element_text(size = 10))
)
}
ggarrange(
box_plot_V1,
box_plot_V2,
box_plot_V3,
box_plot_V4,
box_plot_V5,
box_plot_V6,
box_plot_V7,
box_plot_V8,
box_plot_V9,
box_plot_V10,
box_plot_V11,
box_plot_V12,
box_plot_V13,
box_plot_V14,
box_plot_V15,
box_plot_V16,
box_plot_V17,
box_plot_V18,
box_plot_V19,
box_plot_V20,
box_plot_V21,
box_plot_V22,
box_plot_V23,
box_plot_V24,
box_plot_V25,
box_plot_V26,
box_plot_V27,
box_plot_V28,
box_plot_Amount,
box_plot_Time
)
# Plot the predictors as box plots
predictor_cols <- cols[!cols %in% c('Class')]
for(i in predictor_cols){
assign(paste('box_plot_',i,sep=""),
ggplot() + geom_boxplot(data=df_cc_train, aes_string(x="Class", y=i)) + theme(text = element_text(size = 10))
)
}
ggarrange(
box_plot_V1,
box_plot_V2,
box_plot_V3,
box_plot_V4,
box_plot_V5,
box_plot_V6,
box_plot_V7,
box_plot_V8,
box_plot_V9,
box_plot_V10,
box_plot_V11,
box_plot_V12,
box_plot_V13,
box_plot_V14,
box_plot_V15,
box_plot_V16,
box_plot_V17,
box_plot_V18,
box_plot_V19,
box_plot_V20,
box_plot_V21,
box_plot_V22,
box_plot_V23,
box_plot_V24,
box_plot_V25,
box_plot_V26,
box_plot_V27,
box_plot_V28,
box_plot_Amount,
box_plot_Time
)
# Pulling predictors into a separate dataframe.
preds <- df_cc_train[, -31]
# Scaling the predictors.
preds_scaled <- as.data.frame(scale(preds, center = TRUE, scale = TRUE))
cbind(melt(apply(preds_scaled, 2, min), value.name = "min"),
melt(apply(preds_scaled, 2, mean), value.name = "mean"),
melt(apply(preds_scaled, 2, max), value.name = "max"))
df_cc_train_scaled <- cbind(preds_scaled, Class=df_cc_train$Class)
# Create training set with 75% negative cases and 25% positive cases
# Create dataframe of negative cases
train_neg <- df_cc_train[df_cc_train["Class"] == 0,]
# Sample 1047 negative cases
set.seed(508)
neg_sample <- train_neg[sample(1:nrow(train_neg),1047),]
# Combine with positive cases
downsample_25 <- rbind(neg_sample, df_cc_train[df_cc_train["Class"] == 1,])
table(downsample_25["Class"])
# Created scaled versioin
downsampled_25_scaled <- as.data.frame(scale(downsample_25[,-31], center = TRUE, scale = TRUE))
downsampled_25_scaled <- cbind(downsampled_25_scaled, Class=downsample_25$Class)
validation_true <- df_cc_validation$Class
set.seed(508)
nb.fit <- naiveBayes(Class ~ ., data = downsample_25)
validation_pred <- predict(nb.fit, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.F1 <- nb.confusion[[4]][7])
# Downsampled
set.seed(508)
downsample_50 <- downSample(df_cc_train[,-31], df_cc_train$Class)
# Downsampled and Scaled
set.seed(508)
downsample_scaled <- downSample(df_cc_train_scaled[,-31], df_cc_train_scaled$Class)
set.seed(1)
# SMOTE
smote_cc_train <- SMOTE(X = df_cc_train[, -31], target = df_cc_train$Class, K = 5)
# SMOTE and Scaled
set.seed(1)
smote_cc_train_scaled <- SMOTE(X = df_cc_train_scaled[, -31], target = df_cc_train_scaled$Class, K = 5)
df_cc_validation_scaled_x <- as.data.frame(scale(df_cc_validation[,-31], center = TRUE, scale = TRUE))
df_cc_validation_scaled <- cbind(df_cc_validation_scaled_x, Class=df_cc_validation$Class)
validation_true <- df_cc_validation$Class
set.seed(508)
svc_sample <- smote_cc_train$data[sample(1:nrow(smote_cc_train$data),2000),]
set.seed(508)
tune.out <- tune (svm, as.factor(class) ~ ., data = svc_sample, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
), type = "C"
)
summary(tune.out)
best_svm_smote <- tune.out$best.model
validation_pred <- predict(best_svm_smote, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.FI <- svm.smote.confusion[[4]][7])
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
tree.mod
summary(tree.mod)
library(tree)
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
str(df_cc_train)
tree(Class ~., data = df_cc_train)
detach("package:dplyr", unload=TRUE)
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(caret) # Used for confusionMatrix
library(class)
library(data.table) # Used for melt()
library(dplyr)
library(e1071)
library(ggplot2)
library(ggpubr)
library(glmnet) # Used for Logistic Regression Lasso
library(MASS) # Used for LDA/QDA
library(smotefamily) # Used for SMOTE()
library(tree)
creditcard <- read.csv('creditcard.csv')
creditcard[, 'Class'] <- as.factor(creditcard[, 'Class'])
set.seed(508)
spec = c(train = .7, test = .15, validate = .15)
g = sample(cut(
seq(nrow(creditcard)),
nrow(creditcard)*cumsum(c(0,spec)),
labels = names(spec)
))
res = split(creditcard, g)
# Setting the training dataframe
df_cc_train <- res$train
# Setting the testing dataframe
df_cc_test <- res$test
# Setting the validation dataframe
df_cc_validation <- res$validate
# Pulling predictors into a separate dataframe.
preds <- df_cc_train[, -31]
# Scaling the predictors.
preds_scaled <- as.data.frame(scale(preds, center = TRUE, scale = TRUE))
cbind(melt(apply(preds_scaled, 2, min), value.name = "min"),
melt(apply(preds_scaled, 2, mean), value.name = "mean"),
melt(apply(preds_scaled, 2, max), value.name = "max"))
df_cc_train_scaled <- cbind(preds_scaled, Class=df_cc_train$Class)
########## TREE METHOD ##########
### run initial tree model and summary
tree.mod <- tree(Class ~., data = df_cc_train)
summary(tree.mod) #only use v17 v10 v14 v4 predictors
tree.mod
tree:::print.tree(tree.mod)
### plot tree model
plot(tree.mod)
text(tree.mod, pretty = 0)
### cross-validate to see if best subset model
set.seed(329)
cv.tree.mod <- cv.tree(tree.mod, FUN = prune.misclass)
names(cv.tree.mod)
cv.tree.mod
par(mfrow = c(1,2))
plot(cv.tree.mod$size, cv.tree.mod$dev, type = "b")
plot(cv.tree.mod$k, cv.tree.mod$dev, type = "b")
#best could be 5 or 6 terminal nodes
plot(prune.tree.mod)
### subset original model with 6 terminal nodes
prune.tree.mod <- prune.misclass(tree.mod, best = 6)
plot(prune.tree.mod)
text(prune.tree.mod, pretty = 0)
#this is same model so we'll use orig model
### predict on validation set
tree.pred <- predict(tree.mod, df_cc_validation, type = "class")
(tree.confusion <- confusionMatrix(tree.pred, df_cc_validation_class,
mode="everything", positive = "1"))
df_cc_validation_class <- df_cc_validation$Class
### predict on validation set
tree.pred <- predict(tree.mod, df_cc_validation, type = "class")
(tree.confusion <- confusionMatrix(tree.pred, df_cc_validation_class,
mode="everything", positive = "1"))
(tree.F1 <- tree.confusion[[4]][7])
set.seed(508)
smote_cc_train <- SMOTE(X = dplyr::select(df_cc_train_scaled, -Class), target = df_cc_train_scaled$Class, K = 5)
table(smote_cc_train$data$class)
x.train <- data.matrix(subset(smote_cc_train$data, select = -c(class)))
y.train <- data.matrix(smote_cc_train$data[, c('class')])
logistic.fit <- cv.glmnet(x.train, y.train, family = "binomial", alpha = 1)
bestlam <- logistic.fit$lambda.min
x.validation <- data.matrix(scale(subset(df_cc_validation, select = -c(Class))))
y.validation <- data.matrix(df_cc_validation[, c('Class')])
logistic.preds <- predict(logistic.fit, s = bestlam, newx = x.validation, type = "class")
logistic.confusion <- confusionMatrix(as.factor(logistic.preds), as.factor(y.validation), mode="everything", positive = "1")
logistic.confusion
logistic.F1 <- logistic.confusion[[4]][7]
lda.fit <- lda(class ~ ., data = smote_cc_train$data)
lda.fit
lda.pred <- predict(lda.fit, as.data.frame(x.validation))
lda.class <- lda.pred$class
lda.confusion <- confusionMatrix(as.factor(lda.class), as.factor(y.validation), mode="everything", positive = "1")
lda.confusion
lda.F1 <- lda.confusion[[4]][7]
qda.fit <- qda(class ~ ., data = smote_cc_train$data)
qda.fit
qda.pred <- predict(lda.fit, as.data.frame(x.validation))
qda.class <- qda.pred$class
qda.confusion <- confusionMatrix(as.factor(qda.class), as.factor(y.validation), mode="everything", positive = "1")
qda.confusion
qda.F1 <- qda.confusion[[4]][7]
(svm.smote.F1 <- svm.smote.confusion[[4]][7])
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, nb.orig.F1, nb.scale.F1,
nb.smote.F1, svm.down.F1, svm.down.scale.F1, svm.smote.F1,
svm.smote.scale.F1, tree.F1, knn.F1))
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, svm.down.F1, svm.down.scale.F1, svm.smote.F1,
tree.F1, knn.F1))
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, svm.smote.F1,
tree.F1, knn.F1))
########## KNN Model ##########
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
########## KNN Model ##########
df_cc_train_mat <- df_cc_train[,!names(df_cc_train) %in% c("Class")]
df_cc_train_mat <- scale(df_cc_train_mat)
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
########## KNN Model ##########
df_cc_train_mat <- df_cc_train[,!names(df_cc_train) %in% c("Class")]
df_cc_train_mat <- scale(df_cc_train_mat)
df_cc_validation_mat <- df_cc_validation[, !names(df_cc_validation) %in%
c("Class")]
df_cc_validation_mat <- scale(df_cc_validation_mat)
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
########## KNN Model ##########
df_cc_train_class <- df_cc_train$Class
df_cc_train_mat <- df_cc_train[,!names(df_cc_train) %in% c("Class")]
df_cc_train_mat <- scale(df_cc_train_mat)
df_cc_validation_mat <- df_cc_validation[, !names(df_cc_validation) %in%
c("Class")]
df_cc_validation_mat <- scale(df_cc_validation_mat)
### run knn with prediction
set.seed(506)
knn.pred <- knn(df_cc_train_mat, df_cc_validation_mat, df_cc_train_class, k = 1)
(knn.confusion <- confusionMatrix(knn.pred, df_cc_validation_class,
mode="everything", positive = "1"))
(knn.F1 <- knn.confusion[[4]][7])
##### Model Comparisons by F1 Score #####
(model_table <- rbind(logistic.F1, lda.F1, qda.F1, nb.F1, svm.smote.F1,
tree.F1, knn.F1))
# Two highest F1 scores are tree and knn
### Tree Model
tree.pred.test <- predict(tree.mod, df_cc_test, type = "class")
(tree.test.confusion <- confusionMatrix(tree.pred.test, df_cc_test_class,
mode="everything", positive = "1"))
# split predictors from response for test set
df_cc_test_class <- df_cc_test$Class
df_cc_test_mat <- df_cc_test[, !names(df_cc_test) %in% c("Class")]
df_cc_test_mat <- scale(df_cc_test_mat)
### Tree Model
tree.pred.test <- predict(tree.mod, df_cc_test, type = "class")
(tree.test.confusion <- confusionMatrix(tree.pred.test, df_cc_test_class,
mode="everything", positive = "1"))
(tree.test.F1 <- tree.test.confusion[[4]][7])
### KNN Model
set.seed(506)
knn.pred.test <- knn(df_cc_train_mat, df_cc_test_mat, df_cc_train_class, k = 1)
table(knn.pred.test , df_cc_test_class)
(knn.test.confusion <- confusionMatrix(knn.pred.test, df_cc_test_class,
mode="everything", positive = "1"))
(knn.test.F1 <- knn.test.confusion[[4]][7])
### final comparison
(model_table_final <- rbind(tree.test.F1, knn.test.F1,tree.F1, knn.F1))
##### Model Comparisons by F1 Score #####
(model_table <- rbind(knn.F1, tree.F1, svm.smote.F1, lda.F1, qda.F1, nb.F1, logistic.F1)) # Sorted in descending F1 scores.
# Two highest F1 scores are tree and knn
### final comparison
(model_table_final <- rbind(tree.test.F1, knn.test.F1,tree.F1, knn.F1))
### final comparison
(model_table_final <- rbind(knn.F1, knn.test.F1, tree.F1, tree.test.F1))
set.seed(508)
nb.fit.orig <- naiveBayes(Class ~ ., data = df_cc_train)
validation_pred <- predict(nb.fit.orig, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode="everything", positive = "1")
(nb.orig.confusion <- confusionMatrix(validation_pred, validation_true, mode="everything", positive = "1"))
(nb.orig.F1 <- nb.orig.confusion[[4]][7])
set.seed(508)
nb.fit.scaled <- naiveBayes(Class ~ ., data = downsampled_25_scaled)
validation_pred <- predict(nb.fit.scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.scale.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.scale.F1 <- nb.scale.confusion[[4]][7])
nb.fit.smote <- naiveBayes(class ~ ., data = smote_cc_train$data)
validation_pred <- predict(nb.fit.smote, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.smote <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.smote.F1 <- nb.smote[[4]][7])
nb.fit.smote <- naiveBayes(class ~ ., data = smote_cc_train_scaled$data)
validation_pred <- predict(nb.fit.smote, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(nb.smote <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(nb.smote.F1 <- nb.smote[[4]][7])
# Downsampled
set.seed(508)
downsample_50 <- downSample(df_cc_train[,-31], df_cc_train$Class)
# Downsampled and Scaled
set.seed(508)
downsample_scaled <- downSample(df_cc_train_scaled[,-31], df_cc_train_scaled$Class)
set.seed(1)
# SMOTE
smote_cc_train <- SMOTE(X = df_cc_train[, -31], target = df_cc_train$Class, K = 5)
# SMOTE and Scaled
set.seed(1)
smote_cc_train_scaled <- SMOTE(X = df_cc_train_scaled[, -31], target = df_cc_train_scaled$Class, K = 5)
df_cc_validation_scaled_x <- as.data.frame(scale(df_cc_validation[,-31], center = TRUE, scale = TRUE))
df_cc_validation_scaled <- cbind(df_cc_validation_scaled_x, Class=df_cc_validation$Class)
validation_true <- df_cc_validation$Class
set.seed(508)
tune.out <- tune(svm, Class ~ ., data = downsample_50, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
best_svc_downsample <- tune.out$best.model
validation_pred <- predict(best_svc_downsample, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.down.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.down.F1 <- svm.down.confusion[[4]][7])
set.seed(508)
tune.out <- tune(svm, Class ~ ., data = downsample_scaled, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
best_svc_ds_scaled <- tune.out$best.model
validation_pred <- predict(best_svc_ds_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.down.scale.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.down.scale.F1 <- svm.down.confusion[[4]][7])
set.seed(508)
svc_sample <- smote_cc_train$data[sample(1:nrow(smote_cc_train$data),2000),]
set.seed(508)
tune.out <- tune(svm, as.factor(class) ~ ., data = svc_sample, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)), type = "C")
summary(tune.out)
best_svc_smote <- tune.out$best.model
validation_pred <- predict(best_svc_smote, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.FI <- svm.smote.confusion[[4]][7])
set.seed(508)
svc_sample <- smote_cc_train_scaled$data[sample(1:nrow(smote_cc_train_scaled$data),2000),]
set.seed(508)
tune.out <- tune(svm, as.factor(class) ~ ., data = svc_sample, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)), type = "C")
summary(tune.out)
best_svc_smote_scaled <- tune.out$best.model
validation_pred <- predict(best_svc_smote_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
set.seed(508)
tune.out <- tune (svm, Class ~ ., data = downsample_50, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
)
)
summary(tune.out)
best_svm_downsample <- tune.out$best.model
validation_pred <- predict(best_svm_downsample, df_cc_validation)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.scale.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.scale.F1 <- svm.smote.confusion[[4]][7])
set.seed(508)
tune.out <- tune (svm, Class ~ ., data = downsample_scaled, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
)
)
summary(tune.out)
best_svm_ds_scaled <- tune.out$best.model
validation_pred <- predict(best_svm_ds_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.down.scale.radial.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
#MEK COMMENT: assign F1 score to variable
(svm.down.scale.radial.F1 <- svm.smote.confusion[[4]][7])
set.seed(508)
svc_sample <- smote_cc_train_scaled$data[sample(1:nrow(smote_cc_train_scaled$data),2000),]
set.seed(508)
tune.out <- tune (svm, as.factor(class) ~ ., data = svc_sample, kernel = "radial",
ranges = list (
cost = c(0.1, 1, 5, 10, 100, 1000),
gamma = c(0.5, 1, 2, 3, 4)
), type = "C"
)
summary(tune.out)
best_svm_smote_scaled <- tune.out$best.model
validation_pred <- predict(best_svm_smote_scaled, df_cc_validation_scaled)
confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1")
(svm.smote.scale.radial.confusion <- confusionMatrix(validation_pred, validation_true, mode = "everything", positive = "1"))
(svm.smote.scale.radial.F1 <- svm.smote.confusion[[4]][7])
gc()
CollegeCr <- read.csv('../CollegeCr.csv')
setwd("~/School/STAT 580/Project 2/Code")
CollegeCr <- read.csv('..//CollegeCr.csv')
CollegeCr <- read.csv('../CollegeCr.csv')
CollegeCr <- read.csv('./CollegeCr.csv')
getwd()
CollegeCr <- read.csv('../Data/CollegeCr.csv')
CollegeCr
# Read in the source data files.
CollegeCr <- read.csv('../Data/CollegeCr.csv')
CollegeCr_test <- read.csv('../Data/CollegeCr.test.csv')
Edwards <- read.csv('../Data/Edwards.csv')
Edwards_test <- read.csv('../Data/Edwards.test.csv')
OldTown <- read.csv('../Data/OldTown.csv')
OldTown_test <- read.csv('../Data/OldTown.test.csv')
str(CollegeCr)
cols(CollegeCr)
colnames(CollegeCr)
